This code implements a basic Retrieval-Augmented Generation (RAG) pipeline. It takes a user question, searches the web using DuckDuckGo, fetches content from the top few result URLs, splits the text into manageable chunks, and converts those chunks into vector embeddings stored in a FAISS vector database. When the question is asked, the system retrieves the most relevant text chunks using semantic similarity and passes them as context to a language model, which is instructed to answer only using that retrieved information.It enables an LLM to answer questions using live web data while reducing hallucinations by grounding responses in retrieved sources.
